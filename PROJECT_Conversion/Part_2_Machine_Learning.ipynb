{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0eiKSLYG8XvO"
   },
   "source": [
    "# **Part 2: Machine Learning**\n",
    "\n",
    "# CHALLENGE : predict conversions üèÜüèÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AGhdl7Bt2xZd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W1AU8AH8u0qd",
    "outputId": "00698a97-027b-493b-a2e4-33fdcc295abb"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/elodi/Documents/Documents/PYTHON/Jedha/data_science_full_stack/06_Machine_learning_Supervised/PROJECT_Conversion/Data/conversion_data_train.csv')\n",
    "data = data[data['age'] <= 100]\n",
    "# data_sample = data.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'converted'\n",
    "Y = data.loc[:, target_variable]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to check which variables we will further use to test our models. We know from EDA that total_pages_visited seems the most important variable. So we will try to train our model with only this variable, check the f1 score for the train and test, and then add other variables one by one to see if they improve the f1 scores or not. We first add new_user then age, which seemed the most important after the number of pages visited. In each case, we define a new list of features, then a list of numeric_indices and a list of categorical_indices. Then we define the X dataset to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sjEHMGoY3kMB"
   },
   "outputs": [],
   "source": [
    "# Defining features, and features type (numerical/categorical)\n",
    "features_list = ['total_pages_visited','new_user','age','country','source']\n",
    "numeric_indices = [0,2]\n",
    "categorical_indices = [1,3,4]\n",
    "\n",
    "# Creating several lists of features for our variable study (1 variable, then 2, then 3... then 5)\n",
    "features_lists = [features_list[:i+1] for i in range(len(features_list))]\n",
    "\n",
    "# Locate the X dataset in each subset\n",
    "X_list = [data.loc[:, sublist] for sublist in features_lists]\n",
    "print(\"Dataset with only 1 variable: \")\n",
    "print(X_list[0])\n",
    "print(\"\")\n",
    "print(\"Dataset with 5 variables :\")\n",
    "print(X_list[4])\n",
    "print(\"\")\n",
    "\n",
    "# Numeric and categorical indices for each subset\n",
    "numeric_indices_list = [[i for i in numeric_indices if i in range(len(sublist))] for sublist in features_lists]\n",
    "categorical_indices_list = [[i for i in categorical_indices if i in range(len(sublist))] for sublist in features_lists]\n",
    "\n",
    "for i,num,cat in zip(range(5),numeric_indices_list,categorical_indices_list):\n",
    "    print(\"Subset with \", i, \"variables: Numerical indices: \",num,\"; Categorical indices: \",cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the transformers that will be used during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = StandardScaler() # for numeric features\n",
    "categorical_transformer = OneHotEncoder(drop=\"first\") # for categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each subset (1 or 2 or.. 5 variables), we divide the subset in train and test data, we preprocess them, we run the simplest model, a logistic regression, we make predictions and we assess the model with f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "W8K5DQEvvQgl",
    "outputId": "d280ebc9-4d4b-4723-b9fe-32513f898abc"
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store results\n",
    "variables = []\n",
    "f1_train_scores = []\n",
    "f1_test_scores = []\n",
    "precision_scores = []\n",
    "std_deviations = []\n",
    "\n",
    "for X, numeric_indices, categorical_indices in zip(X_list, numeric_indices_list, categorical_indices_list):\n",
    "    \n",
    "    # Dividing the dataset in train and test (we use stratify as the target variable is not heavenly distributed)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0, stratify=Y)\n",
    "\n",
    "    # Applying pipeline on X variables depending on columns\n",
    "    feature_encoder = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categorical_indices),\n",
    "            ('num', numeric_transformer, numeric_indices)\n",
    "        ]\n",
    "    )\n",
    "    X_train = feature_encoder.fit_transform(X_train)\n",
    "    X_test = feature_encoder.transform(X_test)\n",
    "\n",
    "    # Train the dataset and make predictions\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    Y_train_pred = classifier.predict(X_train)\n",
    "    Y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Calculate scores and precision\n",
    "    f1_train = np.round(f1_score(Y_train, Y_train_pred), 4)\n",
    "    f1_test = np.round(f1_score(Y_test, Y_test_pred), 4)\n",
    "    scores = cross_val_score(classifier, X_train, Y_train, cv=10)\n",
    "    precision = np.round(scores.mean(), 4)\n",
    "    std_dev = np.round(scores.std(), 6)\n",
    "\n",
    "    # Store results in lists\n",
    "    variables.append(len(numeric_indices) + len(categorical_indices))\n",
    "    f1_train_scores.append(f1_train)\n",
    "    f1_test_scores.append(f1_test)\n",
    "    precision_scores.append(precision)\n",
    "    std_deviations.append(std_dev)\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Variables': variables,\n",
    "    'F1 Score (Train)': f1_train_scores,\n",
    "    'F1 Score (Test)': f1_test_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Standard Deviation': std_deviations\n",
    "})\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every variable added to total_pages_visited is improving the f1-score of the train and test sets. So we confirm that we will use the 5 of them when testing our models later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-defining the dataset with the option chosen: 5 variables\n",
    "features_list = ['total_pages_visited','new_user','age','country','source']\n",
    "numeric_indices = [0,2]\n",
    "categorical_indices = [1,3,4]\n",
    "X = data.loc[:, features_list]\n",
    "# I don't need to redefine Y and repreprocess it\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0, stratify=Y)\n",
    "\n",
    "feature_encoder = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_indices),\n",
    "        ('num', numeric_transformer, numeric_indices)\n",
    "    ]\n",
    ")\n",
    "X_train = feature_encoder.fit_transform(X_train)\n",
    "X_test = feature_encoder.transform(X_test) # Don't fit again !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "        'name': 'Logistic Regression',\n",
    "        'model': LogisticRegression(),\n",
    "        'params': \n",
    "            {\n",
    "            'penalty': ['l1', 'l2'],                                    # Regularization type\n",
    "            'C': [0.01, 0.1, 1.0, 10.0, 100, 1000, 5000, 8000],         # Inverse of regularization strength\n",
    "            'solver': ['liblinear', 'lbfgs','newton-cg','sag','saga'],  # Optimization algorithm for smaller datasets\n",
    "            'max_iter': [100, 200, 500, 1000, 5000],                    # Maximum number of iterations to converge\n",
    "            'tol': [1e-3, 1e-4, 1e-5],                                  # Tolerance for stopping criteria\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = [] # This is where we will save all models with metrics\n",
    "\n",
    "grid_search = GridSearchCV(model['model'], model['params'], cv=10, scoring='f1', n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluation and storage of performance in a variable \"results\"\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "train_preds = best_model.predict(X_train)\n",
    "test_preds = best_model.predict(X_test)\n",
    "f1_train = np.round(f1_score(Y_train, train_preds), 4)\n",
    "f1_test = np.round(f1_score(Y_test, test_preds), 4)\n",
    "# scores = cross_val_score(model, X_train, Y_train, cv=10)\n",
    "# precision = np.round(scores.mean(), 4)\n",
    "# std_dev = np.round(scores.std(), 6)\n",
    "accuracy_train = accuracy_score(Y_train, train_preds)\n",
    "accuracy_test = accuracy_score(Y_test, test_preds)\n",
    "recall = recall_score(Y_test, test_preds)\n",
    "auc_roc = roc_auc_score(Y_test, test_preds)\n",
    "\n",
    "# Store the best logistic regression model from the gridsearch\n",
    "results.append({'Model': model['name'],\n",
    "                'F1 Train': f1_train,\n",
    "                'F1 Test': f1_test,\n",
    "                # 'Precision': precision,\n",
    "                # 'Std Dev': std_dev,\n",
    "                'Train Accuracy': accuracy_train,\n",
    "                'Test Accuracy': accuracy_test,\n",
    "                'Recall': recall,\n",
    "                'AUC-ROC': auc_roc,\n",
    "                'Best Model': best_model,\n",
    "                'Best Params': best_params,\n",
    "                'Train Preds': train_preds,\n",
    "                'Test Preds': test_preds\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = grid_search.cv_results_\n",
    "\n",
    "grid_search_results = pd.DataFrame(columns=['Model', 'Mean F1 Score', 'Parameters'])\n",
    "\n",
    "for mean_score, params in zip(cv_results['mean_test_score'], cv_results['params']):\n",
    "    grid_search_results = grid_search_results.append({\n",
    "        'Model': model['name'],\n",
    "        'Mean F1 Score': mean_score,\n",
    "        'Parameters': params\n",
    "    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_logistic_regressions = grid_search_results.sort_values(by='Mean F1 Score', ascending=False)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(sorted_logistic_regressions.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice here that we get smaller f1 scores that we got in the previous part testing the 5 variables (we got 0.7712 for the test). This is explained by the fact that there was no cross validation made before whereas we do a gridsearch with a cv=10 now, which is much more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing more models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to be tested\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        'name': 'SVM Linear',\n",
    "        'model': SVC(kernel='linear', probability=True),\n",
    "        'params': {} \n",
    "    },\n",
    "    {\n",
    "        'name': 'SVM RBF',\n",
    "        'model': SVC(kernel='rbf', probability=True),\n",
    "        'params': {}\n",
    "    },\n",
    "    {\n",
    "        'name': 'SVM Poly (3)',\n",
    "        'model': SVC(kernel='poly', degree=3, probability=True),\n",
    "        'params': {}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Decision Tree Entropy',\n",
    "        'model': DecisionTreeClassifier(criterion=\"entropy\"),\n",
    "        'params': {\n",
    "            'max_depth': [4, 6, 8, 10],\n",
    "            'min_samples_leaf': [1, 2, 5],\n",
    "            'min_samples_split': [2, 4, 8]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Decision Tree Gini',\n",
    "        'model': DecisionTreeClassifier(criterion=\"gini\"),\n",
    "        'params': {\n",
    "            'max_depth': [4, 6, 8, 10],\n",
    "            'min_samples_leaf': [1, 2, 5],\n",
    "            'min_samples_split': [2, 4, 8]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Random Forest',\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [10, 50, 100, 200],\n",
    "            'max_depth': [1, 2, 4, 8],\n",
    "            'min_samples_leaf': [1, 2, 5],\n",
    "            'min_samples_split': [2, 4, 8]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'XGBoost',\n",
    "        'model': XGBClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 150],  # Reduced from [100, 150, 300]\n",
    "            'learning_rate': [0.1, 0.01],  # Reduced from [0.5, 0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "            'max_depth': [4, 5],  # Reduced from [4, 5, 6, 7, 10]\n",
    "            'min_child_weight': [1],  # Only one value\n",
    "            # 'gamma': [0, 0.1, 0.2]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'CatBoost',\n",
    "        'model': CatBoostClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [1000],  # Reduced from [1000, 5000]\n",
    "            'learning_rate': [0.1],  # Reduced from [0.01, 0.05, 0.1, 0.5, 1]\n",
    "            'depth': [6],  # Only one value\n",
    "        }\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Iterate over models, doing a gridsearch for choosing the best params for each model. \n",
    "for model in models:\n",
    "    model_name = model['name']\n",
    "    model_type = model['model']\n",
    "    params = model['params']\n",
    "    grid_search = GridSearchCV(model_type, params, cv=5, scoring='f1', n_jobs=-1)\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "\n",
    "    # Evaluation and storage of performance in a variable \"results\"\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    train_preds = best_model.predict(X_train)\n",
    "    test_preds = best_model.predict(X_test)\n",
    "    f1_train = np.round(f1_score(Y_train, train_preds), 4)\n",
    "    f1_test = np.round(f1_score(Y_test, test_preds), 4)\n",
    "    # scores = cross_val_score(model, X_train, Y_train, cv=10)\n",
    "    # precision = np.round(scores.mean(), 4)\n",
    "    # std_dev = np.round(scores.std(), 6)\n",
    "    accuracy_train = accuracy_score(Y_train, train_preds)\n",
    "    accuracy_test = accuracy_score(Y_test, test_preds)\n",
    "    recall = recall_score(Y_test, test_preds)\n",
    "    auc_roc = roc_auc_score(Y_test, test_preds)\n",
    "\n",
    "    results.append({'Model': model_name,\n",
    "                    'F1 Train': f1_train,\n",
    "                    'F1 Test': f1_test,\n",
    "                    # 'Precision': precision,\n",
    "                    # 'Std Dev': std_dev,\n",
    "                    'Train Accuracy': accuracy_train,\n",
    "                    'Test Accuracy': accuracy_test,\n",
    "                    'Recall': recall,\n",
    "                    'AUC-ROC': auc_roc,\n",
    "                    'Best Model': best_model,\n",
    "                    'Best Params': best_params,\n",
    "                    'Train Preds': train_preds,\n",
    "                    'Test Preds': test_preds\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results storage in a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.iloc[:,:7]\n",
    "sorted_results_df = results_df.sort_values(by='F1 Test', ascending=False)\n",
    "print(sorted_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is still the logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already tuned the logistic regression with the gridsearch. Now, we can try to improve the f1-score using boosting and bagging ensemble learning methods. We will also try voting and stacking ensemble methods, which can mix several kinds of models. For stacking the top3 models from above (logistic regression, xgboost and SVM linear), the time exceeded more than a day, so we decided to try stacking several types of logistic regressions instead, by using the gridsearch results made previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the top3 models tested\n",
    "top3_models = sorted_results_df['Model'].head(3).tolist()\n",
    "print(\"Top 3 models: \",top3_models)\n",
    "top3_result = [result for result in results if result['Model'] in top3_models]\n",
    "\n",
    "# Retrieve interesting models of logistic regressions\n",
    "logistic_regression_top1 = {'C': 8000, 'max_iter': 200, 'penalty': 'l1', 'solver': 'saga', 'tol': 0.001}\n",
    "logistic_regression_top2 = {'C': 10.0, 'max_iter': 200, 'penalty': 'l2', 'solver': 'saga', 'tol': 0.001}\n",
    "logistic_regression_top3 = {'C': 1000, 'max_iter': 5000, 'penalty': 'l2', 'solver': 'saga', 'tol': 0.001}\n",
    "logistic_regression_top4 = \t{'C': 10.0, 'max_iter': 100, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_methods = [\n",
    "    {'name': 'Voting_top3models', 'model': VotingClassifier, 'params': {'estimators': [(model['Model'], model['Best Model']) for model in top3_result]}},\n",
    "    {'name': 'Bagging_20x_logistic_regression_top1', 'model': BaggingClassifier, 'params': {'base_estimator': LogisticRegression(**logistic_regression_top1), 'n_estimators': 20}},\n",
    "    {'name': 'Bagging_100x_logistic_regression_top1', 'model': BaggingClassifier, 'params': {'base_estimator': LogisticRegression(**logistic_regression_top1), 'n_estimators': 100}},\n",
    "    {'name': 'Adaboosting_20x_logistic_regression_top1', 'model': AdaBoostClassifier, 'params': {'base_estimator': LogisticRegression(**logistic_regression_top1), 'n_estimators': 20}},\n",
    "    {'name': 'Adaboosting_100x_logistic_regression_top1', 'model': AdaBoostClassifier, 'params': {'base_estimator': LogisticRegression(**logistic_regression_top1), 'n_estimators': 100}},\n",
    "    {'name': 'Stacking_logistic_regressions', 'model': StackingClassifier, 'params': {'estimators': [\n",
    "        ('Logistic_Regression_Top1', LogisticRegression(**logistic_regression_top1)),\n",
    "        ('Logistic_Regression_Top2', LogisticRegression(**logistic_regression_top2)),\n",
    "        ('Logistic_Regression_Top3', LogisticRegression(**logistic_regression_top3)),\n",
    "        ('Logistic_Regression_Top4', LogisticRegression(**logistic_regression_top4))\n",
    "    ]}},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for ensemble in ensemble_methods:\n",
    "    ensemble_name = ensemble['name']\n",
    "    ensemble_type = ensemble['model']\n",
    "    params = ensemble.get('params', {})\n",
    "    \n",
    "    ensemble_full_model = ensemble_type(**params)\n",
    "    ensemble_full_model.fit(X_train, Y_train)\n",
    "    \n",
    "    train_preds = ensemble_full_model.predict(X_train)\n",
    "    test_preds = ensemble_full_model.predict(X_test)\n",
    "    f1_train = np.round(f1_score(Y_train, train_preds), 4)\n",
    "    f1_test = np.round(f1_score(Y_test, test_preds), 4)\n",
    "    accuracy_train = accuracy_score(Y_train, train_preds)\n",
    "    accuracy_test = accuracy_score(Y_test, test_preds)\n",
    "    recall = recall_score(Y_test, test_preds)\n",
    "    auc_roc = roc_auc_score(Y_test, test_preds)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': ensemble_name, \n",
    "        'F1 Train': f1_train,\n",
    "        'F1 Test': f1_test,\n",
    "        # 'Precision': precision,\n",
    "        # 'Std Dev': std_dev,\n",
    "        'Train Accuracy': accuracy_train,\n",
    "        'Test Accuracy': accuracy_test,\n",
    "        'Recall': recall,\n",
    "        'AUC-ROC': auc_roc,\n",
    "        'Best Model': ensemble_full_model,\n",
    "        'Best Params': params,\n",
    "        'Train Preds': train_preds,\n",
    "        'Test Preds': test_preds\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results storage in a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.iloc[:,:7]\n",
    "sorted_results_df = results_df.sort_values(by='F1 Test', ascending=False)\n",
    "print(sorted_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model_name = \"Bagging_100x_logistic_regression_top1\"\n",
    "\n",
    "# Find the selected model in the results list, retrieve predictions\n",
    "selected_model = next((d for d in results if d[\"Model\"] == selected_model_name), None)\n",
    "Y_train_pred_selected = selected_model[\"Train Preds\"]\n",
    "Y_test_pred_selected = selected_model[\"Test Preds\"]\n",
    "\n",
    "print(\"Confusion matrix on train set:\")\n",
    "print(confusion_matrix(Y_train, Y_train_pred_selected))\n",
    "print(\"Confusion matrix on test set:\")\n",
    "print(confusion_matrix(Y_test, Y_test_pred_selected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the test_dataset and make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate our train and test set (from the first dataset: train_dataset) to train the model on a larger set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "M14RHUadzE2p",
    "outputId": "abcfcfec-9461-4579-adbd-f23270f984eb"
   },
   "outputs": [],
   "source": [
    "X = np.append(X_train,X_test,axis=0)\n",
    "Y = np.append(Y_train,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we retrieve the best models from the whole project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_models_names = sorted_results_df['Model'].head(5).tolist()\n",
    "final_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fit the models on the new larger dataset, retrieve f1 score and other useful metrics.Finally, we take the file \"conversion_data_test\" that we are asked to predict, preprocess it the same way we did with the train_dataset, then we can apply our model on it and predict the labels. We save all models and predictions in the folder \"final_models\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for model_name in top5_models_names:\n",
    "    # Retrieve the model information in the results list\n",
    "    model_info = next(model for model in results if model['Model'] == model_name)\n",
    "    best_model = model_info['Best Model']\n",
    "\n",
    "    # Fit the best model on the new dataset (X, Y)\n",
    "    best_model.fit(X, Y)\n",
    "\n",
    "    # Make predictions on the new dataset\n",
    "    Y_pred = best_model.predict(X)\n",
    "\n",
    "    # Calculate evaluation metrics on the new dataset and store them\n",
    "    f1_final = np.round(f1_score(Y, Y_pred), 4)\n",
    "    accuracy_final = accuracy_score(Y, Y_pred)\n",
    "    recall_final = recall_score(Y, Y_pred)\n",
    "    auc_roc_final = roc_auc_score(Y, Y_pred)\n",
    "    final_results.append({\n",
    "        'Model': model_name,\n",
    "        'F1 Score': f1_final,\n",
    "        'Accuracy': accuracy_final,\n",
    "        'Recall': recall_final,\n",
    "        'AUC-ROC': auc_roc_final\n",
    "    })\n",
    "\n",
    "    # Saving model\n",
    "    joblib.dump(best_model, f'final_models/{model_name}.joblib')\n",
    "\n",
    "    # Predict the the file \"conversion_data_test\"\n",
    "    data_without_labels = pd.read_csv('C:/Users/elodi/Documents/Documents/PYTHON/Jedha/data_science_full_stack/06_Machine_learning_Supervised/PROJECT_Conversion/Data/conversion_data_test.csv') # read file\n",
    "    X_without_labels = data_without_labels.loc[:, features_list].values\n",
    "    X_without_labels = feature_encoder.transform(X_without_labels) # preprocessing\n",
    "    data = {'converted': best_model.predict(X_without_labels)} # predict\n",
    "    Y_predictions = pd.DataFrame(columns=['converted'], data=data) # one column \"converted\", no index\n",
    "    prediction_filename = f'C:/Users/elodi/Documents/Documents/PYTHON/Jedha/data_science_full_stack/06_Machine_learning_Supervised/PROJECT_Conversion/Predictions/{model_name}_predictions.csv'\n",
    "    Y_predictions.to_csv(prediction_filename, index=False) # save into csv file"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Projets_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
